{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63d09d1-a915-462b-a4f5-17b43d852580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "from pdb import set_trace as breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4eb4b3d3-9cef-4597-be54-8b45bd6e288b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540d3fe2b6ad4d44bef3c8f99bc43136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65b31a83b564d9594e756e0f9ef71f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefe66311a8f42769e244118c378b199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f2af6f3d9f49f3b677c21d995081e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e81961b0194534b2019647a0a1108f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"\")\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "DEFAULT_SCENARIOS = {\n",
    "    \"restaurant\": {\"desc\": \"The user is in a restaurant ordering food.\", \"role\": \"A waiter\"},\n",
    "    \"job_interview\": {\"desc\": \"The user is in a job interview for a software engineering position.\", \"role\": \"An interviewer\"},\n",
    "    \"travel\": {\"desc\": \"The user is at an airport checking in for a flight.\", \"role\": \"a check-in agent\"}\n",
    "}\n",
    "\n",
    "\n",
    "PROFILE_DIR = \"user_profiles\"\n",
    "os.makedirs(PROFILE_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a498fe85-def9-4e40-9cfe-68a95f1603ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_profile(username):\n",
    "    profile_path = os.path.join(PROFILE_DIR, f\"{username}.json\")\n",
    "    if os.path.exists(profile_path):\n",
    "        with open(profile_path, \"r\") as file:\n",
    "            user_profile = json.load(file)\n",
    "    else:\n",
    "        user_profile = {\"username\": username, \"scenario\": None, \"chat_history\": []}\n",
    "\n",
    "    if \"ai_role\" not in user_profile:\n",
    "        user_profile[\"ai_role\"] = None\n",
    "\n",
    "    return user_profile\n",
    "\n",
    "\n",
    "def save_user_profile(user_profile):\n",
    "    profile_path = os.path.join(PROFILE_DIR, f\"{user_profile['username']}.json\")\n",
    "    with open(profile_path, \"w\") as file:\n",
    "        json.dump(user_profile, file, indent=4)\n",
    "\n",
    "\n",
    "def format_prompt(user_profile, user_input):\n",
    "    scenario_description = user_profile[\"scenario\"]\n",
    "    chat_history = user_profile[\"chat_history\"]\n",
    "\n",
    "    prompt = f\"[Scenario]: {scenario_description}\\n\\n\"\n",
    "    for entry in chat_history[-5:]:\n",
    "        prompt += f\"### Human: {entry['user']}\\n### Assistant: {entry['ai']}\\n\"\n",
    "    \n",
    "    prompt += f\"### Human: {user_input}\\n### Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def infer_ai_role(scenario_description): # returns suggestions\n",
    "    prompt = f\"Based on the following scenario, what role should the AI play?\\n\\nScenario: {scenario_description}\\n\\nAI Role:\"\n",
    "    print(prompt)\n",
    "    response = llm(\n",
    "        prompt,\n",
    "        do_sample=False, \n",
    "        max_new_tokens=10,\n",
    "    )[0]['generated_text'].split(\"AI Role:\")[-1].strip()\n",
    "    #print(\"AI ROLE RESPONSE\")\n",
    "    #print(response)\n",
    "    return response if response else \"assistant\" \n",
    "\n",
    "def talk_agent(username):\n",
    "    user_profile = load_user_profile(username)\n",
    "\n",
    "    # Select or set a scenario\n",
    "    if not user_profile[\"scenario\"]:\n",
    "        print(\"Choose a scenario:\")\n",
    "        for key in DEFAULT_SCENARIOS:\n",
    "            print(f\"- {key}: {DEFAULT_SCENARIOS[key]['desc']}\")\n",
    "        scenario_key = input(\"Enter scenario name (or type 'custom' to create your own): \").strip().lower()\n",
    "\n",
    "        if scenario_key in DEFAULT_SCENARIOS:\n",
    "            user_profile[\"scenario\"] = DEFAULT_SCENARIOS[scenario_key][\"desc\"]\n",
    "            user_profile[\"ai_role\"] = DEFAULT_SCENARIOS[scenario_key][\"role\"] #Set up role\n",
    "        else:\n",
    "            user_profile[\"scenario\"] = input(\"Describe your custom scenario: \")\n",
    "            user_profile[\"ai_role\"] = infer_ai_role(user_profile[\"scenario\"])\n",
    "        save_user_profile(user_profile)\n",
    "        # print(f\"Scenario set: {user_profile['scenario']}\")\n",
    "        # print(f\"AI will play: {user_profile['ai_role']}\")\n",
    "\n",
    "    else:  # Existing user, infer role if missing\n",
    "        print(\"EXISTING USER\")\n",
    "        if not user_profile.get(\"ai_role\"):\n",
    "            print(\"CREATING AI ROLE\")\n",
    "            user_profile[\"ai_role\"] = infer_ai_role(user_profile[\"scenario\"])\n",
    "            save_user_profile(user_profile)\n",
    "\n",
    "    formatted_prompt = f\"[Scenario]: {user_profile['scenario']}\\n\\n\"\n",
    "    formatted_prompt += f\"### Assistant ({user_profile['ai_role']}):\\n\"\n",
    "    formatted_prompt += \"You are playing the role of the assistant role above in the scenario.\\n\"\n",
    "    formatted_prompt += \"\"\"Start the conversation with you roleplaying as the assistant role.\n",
    "    Your answer must NOT contain any text other than the response. Do NOT copy the prompt into your response.\n",
    "    Post your response here: \n",
    "    \"\"\"\n",
    "    #The above formatting doesn't really do anything...\n",
    "    print(\"\\nChat started! Type 'return' to pause.\\n\")\n",
    "    response = llm(\n",
    "        formatted_prompt,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.7,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.1,\n",
    "        max_new_tokens=100,\n",
    "    )[0]['generated_text'].split('Post your response here:')[-1].strip() #Generate Response\n",
    "    \n",
    "    print(f\"CHATTY:{response}\")\n",
    "    #print(f\"{user_profile['ai_role']}: {response}\")\n",
    "    # print(f\"AI ({user_profile['ai_role']}): {response}\")\n",
    "\n",
    "    user_profile[\"chat_history\"].append({\"user\": \"AI INITIATED\", \"ai\": response})\n",
    "    save_user_profile(user_profile) #Save Response\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == \"return\":\n",
    "            print(\"Chat paused. You can resume later.\")\n",
    "            save_user_profile(user_profile)\n",
    "            break\n",
    "        if user_input.lower() == \"suggest line\":\n",
    "            formatted_prompt = f\"[Scenario]: {user_profile['scenario']}\\n\\n\"\n",
    "            formatted_prompt += f\"### Assistant ({user_profile['ai_role']})\\n\"\n",
    "            formatted_prompt += f\"Give three suggestions to how the user can respond to the following line for the above scenario: {response}\\n\"\n",
    "            formatted_prompt += \"Provide your suggestions in the form of a numbered list with each entry in a separate line. Do NOT include any other text.\"\n",
    "            formatted_prompt += \"Post your response here:\"\n",
    "        else:\n",
    "            formatted_prompt = f\"[Scenario]: {user_profile['scenario']}\\n\\n\"\n",
    "            formatted_prompt += f\"### Assistant ({user_profile['ai_role']})\"\n",
    "            formatted_prompt += \"You are playing the role of the assistant role in the scenario above.\\n \"\n",
    "            formatted_prompt += f\"The user has responded with the following: {user_input}\\n\"\n",
    "            formatted_prompt += \"Continue the conversation by saying only your response.\\nPost your response here:\"\n",
    "        response = llm(\n",
    "            formatted_prompt,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.7,\n",
    "            num_return_sequences=1,\n",
    "            repetition_penalty=1.1,\n",
    "            max_new_tokens=100,\n",
    "            )[0]['generated_text'].split('Post your response here:')[-1].strip()\n",
    "        # breakpoint()\n",
    "        if user_input.lower() == \"suggest line\":\n",
    "            print(f\"Suggested Lines:\\n{response}\")\n",
    "        else:\n",
    "            print(f\"CHATTY:{response}\")\n",
    "            user_profile[\"chat_history\"].append({\"user\": user_input, \"ai\": response})\n",
    "            save_user_profile(user_profile)\n",
    "#suggest line command: have machine suggest a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5403e9c-917a-49b2-be98-1ca44d6bd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your username:  hzt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a scenario:\n",
      "- restaurant: The user is in a restaurant ordering food.\n",
      "- job_interview: The user is in a job interview for a software engineering position.\n",
      "- travel: The user is at an airport checking in for a flight.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter scenario name (or type 'custom' to create your own):  restaurant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chat started! Type 'return' to pause.\n",
      "\n",
      "CHATTY:\"Hello! How may I assist you today?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  suggest line\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Lines:\n",
      "1. Can I help you with something specific?\n",
      "2. Just looking over the menu, thanks.\n",
      "3. What's the most popular dish here?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Just looking over the menu, thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Of course! Is there anything in particular that catches your eye? Our specials today include a delicious grilled salmon and a hearty vegetarian lasagna. Would you like me to tell you more about them?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'm vegan, so do you have any options for me?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Of course! We have several options on our menu that are vegan-friendly. Would you like me to bring you the menu or would you like me to recommend some dishes that fit your dietary restrictions?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'd like some recommendations for vegan dishes!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Absolutely, we have several options on our menu that are vegan-friendly. Our Veggie Burger is a popular choice, made with a black bean patty and topped with avocado, lettuce, and tomato on a whole grain bun. Would you like me to bring that out for you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  The Veggie burger sounds good! Does that have any dairy products in it?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: \"Ah, great choice! Our Veggie Burger doesn't contain any dairy products, so you can enjoy it without any worries. Would you like to try our fries or onion rings as a side dish?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'd love some onion rings!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Great choice! Would you like them plain or with a special sauce?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Can i get the special sauce?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Of course! Our special sauce is a signature item on our menu, made with a blend of tangy and sweet flavors that complement our dishes perfectly. Would you like to try it on your entree or as a dipping sauce for your appetizer?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I'll try it on both the burger and as a dipping sauce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:Assistant: Great! Would you like to try it on our classic cheeseburger or as a dipping sauce for one of our appetizers?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Can I do both?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:\"Of course! We have a variety of options on our menu that can be customized to suit your preferences. Would you like me to recommend some dishes that can be enjoyed together?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  return\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat paused. You can resume later.\n"
     ]
    }
   ],
   "source": [
    "#To do\n",
    "#1. Figure out better chat functions\n",
    "#2. Improve beginning of the prompt\n",
    "#3. Prompt to input for suggestion\n",
    "#4. Return: jumps straight into conversation: maybe do a quick history/review\n",
    "username = input(\"Enter your username: \").strip()\n",
    "talk_agent(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "592a56f4-c4d0-47be-b107-116cd345ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your username:  hzt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXISTING USER\n",
      "\n",
      "Chat started! Type 'return' to pause.\n",
      "\n",
      "CHATTY:\"Hello! How may I assist you today?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  suggest line\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Lines:\n",
      "1. Can I help you with something specific?\n",
      "2. What kind of food are you looking for today?\n",
      "3. Do you have any dietary restrictions or preferences?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What kind of food do you have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATTY:\"We have a wide variety of dishes to choose from, including pasta, pizza, and seafood. Our most popular dish is our signature risotto, made with fresh vegetables and herbs. Would you like me to recommend something?\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  return\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat paused. You can resume later.\n"
     ]
    }
   ],
   "source": [
    "username = input(\"Enter your username: \").strip() #Resume test\n",
    "talk_agent(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64dc68-c896-456d-8b3a-69fef9a7d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
